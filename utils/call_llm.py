from openai import OpenAI
import os
from datetime import datetime

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # dotenv –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—ã—á–Ω—ã–µ env –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ

# Learn more about calling the LLM: https://the-pocket.github.io/PocketFlow/utility_function/llm.html
def call_llm(prompt, system_prompt=None, task_type="default", vision_content=None):    
    """
    –í—ã–∑—ã–≤–∞–µ—Ç LLM —á–µ—Ä–µ–∑ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API (–≤–∫–ª—é—á–∞—è OpenRouter).
    
    Args:
        prompt (str): –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
        system_prompt (str, optional): –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        task_type (str): –¢–∏–ø –∑–∞–¥–∞—á–∏ - "analyzer" –∏–ª–∏ "default"
        vision_content (list, optional): –°–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ Vision API —Ñ–æ—Ä–º–∞—Ç–µ
    
    Returns:
        str: –û—Ç–≤–µ—Ç –æ—Ç LLM
    """
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ .env —Ñ–∞–π–ª–∞
    api_key = os.environ.get("OPENAI_KEY") or os.environ.get("OPENAI_API_KEY", "sk-or-v1-fake-key-for-testing")
    base_url = os.environ.get("OPENAI_URL", "https://openrouter.ai/api/v1")  # –î–ª—è OpenRouter –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
    
    # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
    if task_type == "analyzer":
        model = "qwen/qwen2.5-vl-32b-instruct:free"
    else:
        model = "deepseek/deepseek-r1-0528:free"
    
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ URL
    client = OpenAI(api_key=api_key, base_url=base_url)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    user_message = {"role": "user", "content": []}
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç
    if prompt:
        user_message["content"].append({
            "type": "text",
            "text": prompt
        })
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
    if vision_content:
        user_message["content"].extend(vision_content)
    
    # –ï—Å–ª–∏ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç
    if not vision_content:
        user_message = {"role": "user", "content": prompt}
    
    messages.append(user_message)
    
    # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
    print(f"‚è≥ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ {model}...")
    start_time = datetime.now()
    
    try:
        # –í—ã–∑—ã–≤–∞–µ–º LLM —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        r = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0
        )
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ LLM: {e}")
        return ""
    
    end_time = datetime.now()
    response_time = (end_time - start_time).total_seconds()
    
    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
    response_text = ""
    try:
        if getattr(r, "choices", None):
            first_choice = r.choices[0]
            if first_choice and getattr(first_choice, "message", None):
                response_text = first_choice.message.content or ""
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏: {e}")
        response_text = ""
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –∑–∞ {response_time:.2f} —Å–µ–∫ (–º–æ–¥–µ–ª—å: {model})")
    
    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∞—Ö
    try:
        if hasattr(r, 'usage') and r.usage:
            total_tokens = getattr(r.usage, 'total_tokens', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            print(f"üìä –¢–æ–∫–µ–Ω—ã: {total_tokens}")
        else:
            print("üìä –¢–æ–∫–µ–Ω—ã: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
    except Exception as e:
        print(f"üìä –¢–æ–∫–µ–Ω—ã: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ (–æ—à–∏–±–∫–∞: {e})")
    
    if response_text:
        print(f"üìù –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(response_text)} —Å–∏–º–≤–æ–ª–æ–≤, {len(response_text.split())} —Å–ª–æ–≤")
    else:
        print("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏")
    
    # –ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
    if response_text and len(response_text.split()) < 5:
        print(f"‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ü–æ–ª—É—á–µ–Ω –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ {model}")
        print(f"   –û—Ç–≤–µ—Ç: '{response_text}'")
    
    return response_text
    
if __name__ == "__main__":
    # –¢–µ—Å—Ç —Å –æ–±—ã—á–Ω–æ–π –∑–∞–¥–∞—á–µ–π
    prompt = "What is the meaning of life?"
    print("Default model:", call_llm(prompt))
    
    # –¢–µ—Å—Ç —Å –∑–∞–¥–∞—á–µ–π –∞–Ω–∞–ª–∏–∑–∞
    analyzer_prompt = "Analyze this image content"
    print("Analyzer model:", call_llm(analyzer_prompt, task_type="analyzer"))
